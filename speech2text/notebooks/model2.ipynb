{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub ##for loading full tf model\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import librosa  \n",
    "import soundfile as sf ##may not be needed\n",
    "import os, json, re\n",
    "from itertools import groupby\n",
    "\n",
    "from scipy.io import wavfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = '../models/lite-model_ASR_TFLite_pre_trained_models_English_1.tflite'\n",
    "audio = '../test_audio/recording2.wav'\n",
    "signal, sr = librosa.load(audio, sr=16000, mono=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\u0000\u0000\u0000she had your \u0000\u0000\u0000du\u0000\u0000ck \u0000su\u0000\u0000e \u0000\u0000and greasy\u0000\u0000 \u0000\u0000\u0000wat\u0000ch \u0000for all year\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "interpreter.resize_tensor_input(input_details[0][\"index\"], signal.shape)\n",
    "interpreter.allocate_tensors()\n",
    "interpreter.set_tensor(input_details[0][\"index\"], signal)\n",
    "interpreter.set_tensor(\n",
    "    input_details[1][\"index\"],\n",
    "    np.array(0).astype('int32')\n",
    ")\n",
    "interpreter.set_tensor(\n",
    "    input_details[2][\"index\"],\n",
    "    np.zeros([1,2,1,320]).astype('float32')\n",
    ")\n",
    "interpreter.invoke()\n",
    "hyp = interpreter.get_tensor(output_details[0][\"index\"])\n",
    "\n",
    "print(\"\".join([chr(u) for u in hyp]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_to_text:\n",
    "    def __init__(self):\n",
    "        self.create_mappings()\n",
    "\n",
    "    def create_mappings(self):\n",
    "        self.token_to_id_mapping = self._get_vocab() #character to ascii int\n",
    "\n",
    "        self.id_to_token_mapping = {v: k for k, v in self.token_to_id_mapping.items()} #ascii int to character\n",
    "\n",
    "        self.special_tokens = [\"<pad>\"]\n",
    "        self.special_ids = [self.token_to_id_mapping[k] for k in self.special_tokens]\n",
    "\n",
    "    def _get_vocab(self):\n",
    "        alph = [\" \", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "        ascii_int = [ord(c) for c in alph] \n",
    "        vocab = {c : i for i,c in zip(ascii_int, alph)} ##token to id mapping\n",
    "        vocab[\"<pad>\"] = 0\n",
    "        return vocab\n",
    "\n",
    "    def decode(self, input_ids: list, skip_special_tokens=True, group_tokens=True):\n",
    "        \"\"\"\n",
    "        Use this method to decode your ids back to string.\n",
    "        Args:\n",
    "            input_ids (:obj: `list`):\n",
    "                input_ids you want to decode to string.\n",
    "            skip_special_tokens (:obj: `bool`, `optional`):\n",
    "                Whether to remove special tokens (like `<pad>`) from string.\n",
    "            group_tokens (:obj: `bool`, `optional`):\n",
    "                Whether to group repeated characters.\n",
    "        \"\"\"\n",
    "        if group_tokens:\n",
    "            input_ids = [t[0] for t in groupby(input_ids)]\n",
    "        if skip_special_tokens:\n",
    "            input_ids = [k for k in input_ids if k not in self.special_ids]\n",
    "        tokens = [self.id_to_token_mapping.get(k, \"<unk>\") for k in input_ids]\n",
    "        tokens = [k if k not in self.special_tokens else \"\" for k in tokens]\n",
    "        return \"\".join(tokens).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = model_to_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0, 115, 104, 101,  32, 104,  97, 100,  32, 121,\n",
       "       111, 117, 114,  32,   0,   0,   0, 100, 117,   0,   0,  99, 107,\n",
       "        32,   0, 115, 117,   0,   0, 101,  32,   0,   0,  97, 110, 100,\n",
       "        32, 103, 114, 101,  97, 115, 121,   0,   0,  32,   0,   0,   0,\n",
       "       119,  97, 116,   0,  99, 104,  32,   0, 102, 111, 114,  32,  97,\n",
       "       108, 108,  32, 121, 101,  97, 114])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'she had your duck sue and greasy watch for al year'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.decode(hyp.tolist(), skip_special_tokens=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "68b7c336e82da17fec47fda1173db237956d426c465f0139b547f593f066502d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
