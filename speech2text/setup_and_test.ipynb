{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub ##for loading full tf model\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import librosa  \n",
    "import soundfile as sf ##may not be needed\n",
    "import os, json, re\n",
    "from itertools import groupby\n",
    "\n",
    "from scipy.io import wavfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Conversion\n",
    "The only available speech recognition tflite model available is not very good. We can try to convert a better model to tflite.  \n",
    "I'll try to use [this one](https://tfhub.dev/vasudevgupta7/wav2vec2-960h/1) which is a finetuned wav2vec2 model.  \n",
    "The results are much better, and we could even use code from [this repo](https://github.com/thevasudevgupta/gsoc-wav2vec2) to finetune our own wave2vec2 model.  \n",
    "\n",
    "However, while inference only takes a few seconds on my latop's CPU, it's very slow on the Raspberry Pi. We can utilize [post-training quantization](https://www.tensorflow.org/lite/performance/post_training_quantization) to reduce the size and inference time of a TFLite model without much reduction to accuracy.   \n",
    "After reading through Tensorflow's guide, it seems like dynamic range quantization is the best fit for this project.  \n",
    "All we have to do is add `converter.optimizations = [tf.lite.Optimize.DEFAULT]` after we initialize the TFLite converter. The final .TFLITE file which holds the saved model is 92,977 KB while without quantization it is 369,106 KB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full tf model from tensorflow hub (about 1m 39s)\n",
    "wav2vec_url = \"https://tfhub.dev/vasudevgupta7/wav2vec2-960h/1\"\n",
    "full_model = hub.load(wav2vec_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 342). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tf_wave2vec\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tf_wave2vec\\assets\n"
     ]
    }
   ],
   "source": [
    "# Save the model to the 'saved model' format, which is the recommended format to convert from\n",
    "tf.saved_model.save(full_model, 'tf_wave2vec') ##(about 1m 24s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use converter to convert model to tflite (about 2m 30s)\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('./tf_wave2vec/')\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tflite model\n",
    "with open('quant-wave2vec2-960h.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)\n",
    "\n",
    "\n",
    "# Remove tensorflow saved model directory if desired..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./models/quant-wave2vec2-960h.tflite\"\n",
    "\n",
    "wav_path = \"./test_audio/recording.wav\"\n",
    "REQUIRED_SAMPLE_RATE = 16000\n",
    "MAX_LENGTH = 246000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153920,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal, sample_rate = librosa.load(wav_path, sr=REQUIRED_SAMPLE_RATE, mono=True)\n",
    "signal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_details = [{'name': 'serving_default_input_1:0', 'index': 0, 'shape': array([    1, 50000]), 'shape_signature': array([   -1, 50000]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "\n",
      "output_details = [{'name': 'StatefulPartitionedCall:0', 'index': 1347, 'shape': array([ 1,  1, 32]), 'shape_signature': array([-1, -1, 32]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "print(f\"{input_details = }\")\n",
    "print(\"\")\n",
    "print(f\"{output_details = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_pad(x, pad=True):\n",
    "  \"\"\"\n",
    "  Normalize and pad input signal to match preprocessing of the model.\n",
    "  Methodology from: https://github.com/thevasudevgupta/gsoc-wav2vec2/blob/main/src/wav2vec2/processor.py\n",
    "  \"\"\"\n",
    "  MAX_LENGTH = 246000 ##https://tfhub.dev/vasudevgupta7/wav2vec2-960h/1\n",
    "  #normalize()\n",
    "  mean = np.mean(x, axis=-1, keepdims=True)\n",
    "  var = np.var(x, axis=-1, keepdims=True)\n",
    "  x = np.squeeze((x - mean) / np.sqrt(var + 1e-5))\n",
    "  #pad\n",
    "  if pad:\n",
    "    padding = np.zeros(MAX_LENGTH - x.shape[0])\n",
    "    x = np.concatenate((x, padding))\n",
    "  return x\n",
    "\n",
    "def resize_input_seq(interpreter, speech):\n",
    "  \"Resize the input signal to the size that the model will accept\"\n",
    "  _, seq_length = interpreter.get_input_details()[0]['shape']\n",
    "  speech = np.resize(speech, (1, seq_length))\n",
    "  speech = speech.astype(np.float32)\n",
    "  return speech\n",
    "\n",
    "def set_input_tensor(interpreter, speech):\n",
    "  tensor_index = interpreter.get_input_details()[0]['index']\n",
    "  input_tensor = interpreter.tensor(tensor_index)()[0]\n",
    "  input_tensor[:] = speech\n",
    "\n",
    "def classify_speech(interpreter, speech):\n",
    "  speech = normalize_pad(speech, pad=True)\n",
    "  speech = resize_input_seq(interpreter, speech)\n",
    "  # Setting the input tensor and invoking the interpreter runs the inference\n",
    "  set_input_tensor(interpreter, speech)\n",
    "  interpreter.invoke()\n",
    "  output_details = interpreter.get_output_details()[0]\n",
    "  output = np.squeeze(interpreter.get_tensor(output_details['index']))\n",
    "  \n",
    "  return speech, np.squeeze(np.argmax(output, axis=-1))    \n",
    "\n",
    "\n",
    "\n",
    "input, output = classify_speech(interpreter, signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50000)\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "print(input.shape) ##final shape of the input tensor\n",
    "print(input.dtype) ##dtype of input tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0, 15,  0,  0,  0,  0,  0,  8,  0,  0,\n",
       "        0,  4,  0,  0,  6, 11,  0,  0,  5,  5,  0,  0, 13, 13,  5,  0,  4,\n",
       "        4,  4,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0], dtype=int64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(vocab_path):\n",
    "    with open(vocab_path, \"r\") as f:\n",
    "        vocab = json.load(f)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "token_to_id_mapping = get_vocab(\"./vocab.json\")\n",
    "id_to_token_mapping = {v: k for k, v in token_to_id_mapping.items()}\n",
    "\n",
    "unk_token = \"<unk>\"\n",
    "unk_id = token_to_id_mapping[unk_token]\n",
    "\n",
    "dimiliter_token = \"|\"\n",
    "dimiliter_id = token_to_id_mapping[dimiliter_token]\n",
    "\n",
    "special_tokens = [\"<pad>\"]\n",
    "special_ids = [token_to_id_mapping[k] for k in special_tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LO THERE'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode(input_ids: list, skip_special_tokens=True, group_tokens=True):\n",
    "    \"\"\"\n",
    "    Use this method to decode your ids back to string.\n",
    "    Args:\n",
    "        input_ids (:obj: `list`):\n",
    "            input_ids you want to decode to string.\n",
    "        skip_special_tokens (:obj: `bool`, `optional`):\n",
    "            Whether to remove special tokens (like `<pad>`) from string.\n",
    "        group_tokens (:obj: `bool`, `optional`):\n",
    "            Whether to group repeated characters.\n",
    "    \"\"\"\n",
    "    if group_tokens:\n",
    "        input_ids = [t[0] for t in groupby(input_ids)]\n",
    "    if skip_special_tokens:\n",
    "        input_ids = [k for k in input_ids if k not in special_ids]\n",
    "    tokens = [id_to_token_mapping.get(k, unk_token) for k in input_ids]\n",
    "    tokens = [k if k != dimiliter_token else \" \" for k in tokens]\n",
    "    return \"\".join(tokens).strip()\n",
    "\n",
    "decode(output.tolist(), True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model works much better but it is quite a bit larger - I cannot upload to github."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "68b7c336e82da17fec47fda1173db237956d426c465f0139b547f593f066502d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
